<!doctype html>
<html data-n-head-ssr lang="en" data-n-head="%7B%22lang%22:%7B%22ssr%22:%22en%22%7D%7D">
  <head>
    <title>ChrisYoon</title><meta data-n-head="ssr" charset="utf-8"><meta data-n-head="ssr" name="viewport" content="width=device-width,initial-scale=1"><meta data-n-head="ssr" data-hid="description" name="description" content="Blog"><meta data-n-head="ssr" data-hid="mobile-web-app-capable" name="mobile-web-app-capable" content="yes"><meta data-n-head="ssr" data-hid="apple-mobile-web-app-title" name="apple-mobile-web-app-title" content="ChrisYoon"><meta data-n-head="ssr" data-hid="author" name="author" content="Chris Yoon"><meta data-n-head="ssr" data-hid="theme-color" name="theme-color" content="#fff"><meta data-n-head="ssr" data-hid="og:type" name="og:type" property="og:type" content="website"><meta data-n-head="ssr" data-hid="og:title" name="og:title" property="og:title" content="ChrisYoon"><meta data-n-head="ssr" data-hid="og:site_name" name="og:site_name" property="og:site_name" content="ChrisYoon"><meta data-n-head="ssr" data-hid="og:description" name="og:description" property="og:description" content="Blog"><link data-n-head="ssr" rel="stylesheet" href="https://fonts.googleapis.com/css2?family=PT+Serif:ital,wght@0,400;0,700;1,700&family=Poppins:wght@500;700&display=swap"><link data-n-head="ssr" rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.0/dist/katex.min.css"><link data-n-head="ssr" rel="manifest" href="/_nuxt/manifest.114d7875.json"><link data-n-head="ssr" rel="shortcut icon" href="/_nuxt/icons/icon_64.5f6a36.png"><link data-n-head="ssr" rel="apple-touch-icon" href="/_nuxt/icons/icon_512.5f6a36.png" sizes="512x512"><link rel="preload" href="/_nuxt/149e7f33763fef6bbd60.js" as="script"><link rel="preload" href="/_nuxt/8df7c34f58155934d1b3.js" as="script"><link rel="preload" href="/_nuxt/2f2445117952219408c1.js" as="script"><link rel="preload" href="/_nuxt/d1d05cc850efa9e8bee5.js" as="script"><link rel="preload" href="/_nuxt/75fb1812f7eecc3449f1.js" as="script"><style data-vue-ssr-id="38dfa7e4:0 517a8dd7:0 3191d5ad:0 932a8f60:0 453d1afc:0 e309b01c:0 5807cbe2:0">/*! normalize.css v8.0.1 | MIT License | github.com/necolas/normalize.css */html{line-height:1.15;-webkit-text-size-adjust:100%}body{margin:0}main{display:block}h1{font-size:2em;margin:.67em 0}hr{box-sizing:content-box;height:0;overflow:visible}pre{font-family:monospace,monospace;font-size:1em}a{background-color:transparent}abbr[title]{border-bottom:none;text-decoration:underline;-webkit-text-decoration:underline dotted;text-decoration:underline dotted}b,strong{font-weight:bolder}code,kbd,samp{font-family:monospace,monospace;font-size:1em}small{font-size:80%}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}sub{bottom:-.25em}sup{top:-.5em}img{border-style:none}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0}button,input{overflow:visible}button,select{text-transform:none}[type=button],[type=reset],[type=submit],button{-webkit-appearance:button}[type=button]::-moz-focus-inner,[type=reset]::-moz-focus-inner,[type=submit]::-moz-focus-inner,button::-moz-focus-inner{border-style:none;padding:0}[type=button]:-moz-focusring,[type=reset]:-moz-focusring,[type=submit]:-moz-focusring,button:-moz-focusring{outline:1px dotted ButtonText}fieldset{padding:.35em .75em .625em}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal}progress{vertical-align:baseline}textarea{overflow:auto}[type=checkbox],[type=radio]{box-sizing:border-box;padding:0}[type=number]::-webkit-inner-spin-button,[type=number]::-webkit-outer-spin-button{height:auto}[type=search]{-webkit-appearance:textfield;outline-offset:-2px}[type=search]::-webkit-search-decoration{-webkit-appearance:none}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit}details{display:block}summary{display:list-item}[hidden],template{display:none}blockquote,dd,dl,figure,h1,h2,h3,h4,h5,h6,hr,p,pre{margin:0}button{background-color:transparent;background-image:none;padding:0}button:focus{outline:1px dotted;outline:5px auto -webkit-focus-ring-color}fieldset,ol,ul{margin:0;padding:0}ol,ul{list-style:none}html{font-family:system-ui,-apple-system,Segoe UI,Roboto,Ubuntu,Cantarell,Noto Sans,sans-serif,BlinkMacSystemFont,Helvetica Neue,Arial,Apple Color Emoji,Segoe UI Emoji,Segoe UI Symbol,Noto Color Emoji;line-height:1.5}*,:after,:before{box-sizing:border-box;border:0 solid #e2e8f0}hr{border-top-width:1px}img{border-style:solid}textarea{resize:vertical}input::-webkit-input-placeholder,textarea::-webkit-input-placeholder{color:#a0aec0}input::-moz-placeholder,textarea::-moz-placeholder{color:#a0aec0}input:-ms-input-placeholder,textarea:-ms-input-placeholder{color:#a0aec0}input::-ms-input-placeholder,textarea::-ms-input-placeholder{color:#a0aec0}input::placeholder,textarea::placeholder{color:#a0aec0}[role=button],button{cursor:pointer}table{border-collapse:collapse}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit}a{color:inherit;text-decoration:inherit}button,input,optgroup,select,textarea{padding:0;line-height:inherit;color:inherit}code,kbd,pre,samp{font-family:Menlo,Monaco,Consolas,Liberation Mono,Courier New,monospace}audio,canvas,embed,iframe,img,object,svg,video{display:block;vertical-align:middle}img,video{max-width:100%;height:auto}.container{width:100%}@media (min-width:640px){.container{max-width:640px}}@media (min-width:768px){.container{max-width:768px}}@media (min-width:1024px){.container{max-width:1024px}}@media (min-width:1280px){.container{max-width:1280px}}.bg-white{--bg-opacity:1;background-color:#fff;background-color:rgba(255,255,255,var(--bg-opacity))}.border-gray-200{--border-opacity:1;border-color:#edf2f7;border-color:rgba(237,242,247,var(--border-opacity))}.border-gray-300{--border-opacity:1;border-color:#e2e8f0;border-color:rgba(226,232,240,var(--border-opacity))}.hover\:border-gray-400:hover{--border-opacity:1;border-color:#cbd5e0;border-color:rgba(203,213,224,var(--border-opacity))}.rounded{border-radius:.25rem}.rounded-lg{border-radius:.5rem}.border-2{border-width:2px}.border{border-width:1px}.flex{display:flex}.flex-row{flex-direction:row}.flex-col{flex-direction:column}.flex-wrap{flex-wrap:wrap}.items-center{align-items:center}.justify-center{justify-content:center}.justify-between{justify-content:space-between}.flex-1{flex:1 1 0%}.font-bold{font-weight:700}.h-10{height:2.5rem}.h-32{height:8rem}.text-sm{font-size:.875rem}.text-base{font-size:1rem}.text-xl{font-size:1.25rem}.text-2xl{font-size:1.5rem}.text-3xl{font-size:1.875rem}.my-2{margin-top:.5rem;margin-bottom:.5rem}.mx-2{margin-left:.5rem;margin-right:.5rem}.my-4{margin-top:1rem;margin-bottom:1rem}.mx-6{margin-left:1.5rem;margin-right:1.5rem}.my-8{margin-top:2rem;margin-bottom:2rem}.mx-auto{margin-left:auto;margin-right:auto}.mr-2{margin-right:.5rem}.mb-2{margin-bottom:.5rem}.mt-3{margin-top:.75rem}.mb-3{margin-bottom:.75rem}.mt-4{margin-top:1rem}.mt-5{margin-top:1.25rem}.mb-6{margin-bottom:1.5rem}.mb-8{margin-bottom:2rem}.mr-auto{margin-right:auto}.ml-auto{margin-left:auto}.min-h-full{min-height:100%}.min-h-screen{min-height:100vh}.object-contain{-o-object-fit:contain;object-fit:contain}.object-center{-o-object-position:center;object-position:center}.overflow-hidden{overflow:hidden}.py-1{padding-top:.25rem;padding-bottom:.25rem}.py-3{padding-top:.75rem;padding-bottom:.75rem}.py-4{padding-top:1rem;padding-bottom:1rem}.px-5{padding-left:1.25rem;padding-right:1.25rem}.px-6{padding-left:1.5rem;padding-right:1.5rem}.px-32{padding-left:8rem;padding-right:8rem}.pt-2{padding-top:.5rem}.pt-10{padding-top:2.5rem}.pr-16{padding-right:4rem}.static{position:static}.relative{position:relative}.shadow-lg{box-shadow:0 10px 15px -3px rgba(0,0,0,.1),0 4px 6px -2px rgba(0,0,0,.05)}.hover\:shadow:hover{box-shadow:0 1px 3px 0 rgba(0,0,0,.1),0 1px 2px 0 rgba(0,0,0,.06)}.text-left{text-align:left}.text-center{text-align:center}.text-gray-600{--text-opacity:1;color:#718096;color:rgba(113,128,150,var(--text-opacity))}.text-gray-700{--text-opacity:1;color:#4a5568;color:rgba(74,85,104,var(--text-opacity))}.text-gray-900{--text-opacity:1;color:#1a202c;color:rgba(26,32,44,var(--text-opacity))}.italic{font-style:italic}.visible{visibility:visible}.w-1\/3{width:33.333333%}.w-2\/3{width:66.666667%}.w-full{width:100%}.transition{transition-property:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform}@media (min-width:640px){.sm\:w-full{width:100%}}@media (min-width:1024px){.lg\:flex{display:flex}.lg\:max-w-full{max-width:100%}.lg\:w-1\/3{width:33.333333%}}code[class*=language-],pre[class*=language-]{color:#000;background:0 0;text-shadow:0 1px #fff;font-family:Consolas,Monaco,Andale Mono,Ubuntu Mono,monospace;font-size:1em;text-align:left;white-space:pre;word-spacing:normal;word-break:normal;word-wrap:normal;line-height:1.5;-moz-tab-size:4;-o-tab-size:4;tab-size:4;-webkit-hyphens:none;-ms-hyphens:none;hyphens:none}code[class*=language-] ::-moz-selection,code[class*=language-]::-moz-selection,pre[class*=language-] ::-moz-selection,pre[class*=language-]::-moz-selection{text-shadow:none;background:#b3d4fc}code[class*=language-] ::selection,code[class*=language-]::selection,pre[class*=language-] ::selection,pre[class*=language-]::selection{text-shadow:none;background:#b3d4fc}@media print{code[class*=language-],pre[class*=language-]{text-shadow:none}}pre[class*=language-]{padding:1em;margin:.5em 0;overflow:auto}:not(pre)>code[class*=language-],pre[class*=language-]{background:#f5f2f0}:not(pre)>code[class*=language-]{padding:.1em;border-radius:.3em;white-space:normal}.token.cdata,.token.comment,.token.doctype,.token.prolog{color:#708090}.token.punctuation{color:#999}.token.namespace{opacity:.7}.token.boolean,.token.constant,.token.deleted,.token.number,.token.property,.token.symbol,.token.tag{color:#905}.token.attr-name,.token.builtin,.token.char,.token.inserted,.token.selector,.token.string{color:#690}.language-css .token.string,.style .token.string,.token.entity,.token.operator,.token.url{color:#9a6e3a;background:hsla(0,0%,100%,.5)}.token.atrule,.token.attr-value,.token.keyword{color:#07a}.token.class-name,.token.function{color:#dd4a68}.token.important,.token.regex,.token.variable{color:#e90}.token.bold,.token.important{font-weight:700}.token.italic{font-style:italic}.token.entity{cursor:help}.nuxt-progress{position:fixed;top:0;left:0;right:0;height:2px;width:0;opacity:1;transition:width .1s,opacity .4s;background-color:#fff;z-index:999999}.nuxt-progress.nuxt-progress-notransition{transition:none}.nuxt-progress-failed{background-color:red}html{font-size:16px;word-spacing:1px;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased;box-sizing:border-box;overflow-y:scroll;background-color:#f5f5f5}*,:after,:before{box-sizing:border-box;margin:0}.content{margin-left:auto;margin-right:auto;padding-left:2rem;padding-right:2rem;max-width:640px}.navbar{padding:.75rem 1.25rem;border-bottom:2px solid #ebedf0;color:#2b3d4f;font-family:Poppins,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,sans-serif}@media screen and (min-width:600px){.navbar{display:flex;justify-content:space-between;align-items:center}}.navbar-item{margin-right:1.25rem;font-size:1.2rem;font-weight:500}.navbar-item:last-child{margin-right:0}.navbar-item:hover{border-bottom:2px solid #33b1ff}.navbar-item-wrapper{text-align:center;padding-top:.5em}.navbar-logo{display:flex;align-items:center;margin-left:auto;margin-right:auto;margin-bottom:.75rem;font-size:1.3rem;font-weight:600;padding-top:.5em}.navbar-logo:hover{color:#33b1ff}@media screen and (min-width:600px){.navbar-logo{margin-bottom:0}}.navbar-logo-image{display:inline-block;width:36px;margin-right:.75rem}.nuxt-link-exact-active{border-bottom:2px solid #33b1ff}.blog-title{font-family:Poppins,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,sans-serif;color:#2b3d4f;font-weight:700;font-size:1.875rem;margin-top:2rem}.content{margin-left:auto;margin-right:auto;padding-left:2rem;padding-right:2rem;max-width:1000px}.nuxt-content h2{color:#2b3d4f;font-family:Poppins,-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Helvetica Neue,Arial,sans-serif;font-weight:700;margin-top:1.25rem;margin-bottom:1.25rem;padding-bottom:.75rem;border-bottom:1px solid #ebedf0;font-size:1.25rem;line-height:.5}.nuxt-content li{line-height:1.7;font-size:16px;font-family:"PT Serif",serif}@media screen and (min-width:600px){.nuxt-content li{font-size:18px}}.nuxt-content p{margin-bottom:1rem}.nuxt-content ol,.nuxt-content ul{list-style-position:outside;list-style-type:decimal;list-style-position:inside;margin-bottom:1rem}.footer{text-align:center;font-family:"PT Serif",serif;font-size:.875rem;margin-top:3rem;padding:2rem;border-top:2px solid #ebedf0}</style>
  </head>
  <body>
    <div data-server-rendered="true" id="__nuxt"><!----><div id="__layout"><div class="flex flex-col min-h-screen"><nav class="navbar"><div class="flex items-center"><a href="/" class="navbar-logo nuxt-link-active">
      Chris Yoon
    </a></div> <div class="navbar-item-wrapper"><a href="/" class="navbar-item nuxt-link-active">Home</a> <a href="/about" class="navbar-item">About</a></div></nav> <main class="flex-1"><article class="content"><h1 class="blog-title">Follow-up Thoughts on a Couple Previous Publications from Dr. Shuran Song Lab.</h1> <h3 class="text-gray-600 mt-4 mb-8 text-italics">
      September 3, 2020
    </h3> <div class="nuxt-content"><p>I have read the papers "Spatial Action Maps for Mobile Manipulation" and "Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning." I have summarized a few of my follow-up thoughts below.</p>
<h3 id="1-distributional-value-approximations-embedded-into-pixel-wise-output-map"><a aria-hidden="true" href="#1-distributional-value-approximations-embedded-into-pixel-wise-output-map" tabindex="-1"><span class="icon icon-link"></span></a>1) Distributional value approximations embedded into pixel-wise output map</h3>
<p>Distributional value approximation [1, 2, 3] works great with deep reinforcement learning. I am wondering if these performance gains can be translated to learning pixel-wise value maps. If the added computational costs are too prohibitive (e.g. the output channel will now be multiple channels deep, each representing a single discretized unit of the value distribution), how can we make it more efficient? </p>
<p>Another reason for considering distributional value approximation is that, intuitively, it can introduce "risk sensitive" behavior, allowing agents to act conservatively when a certain state-action pair is estimated to have a high variance value distribution. To test this concept, how can we design similar motion-primitive or mobile navigation based tasks that involve risk sensitivity?</p>
<h3 id="2-can-we-learn-synergies-offline"><a aria-hidden="true" href="#2-can-we-learn-synergies-offline" tabindex="-1"><span class="icon icon-link"></span></a>2) Can we learn synergies offline?</h3>
<p>I've noticed that offline RL has recently gained a lot of momentum in RL research [5]. Given a rich data set of transitions including multiple motion primitives, can a policy learn synergies between multiple motion primitives without any interaction with the environment (starting with simulations, but eventually moving onto sim-to-real)? </p>
<p>It would be particularly interesting to benchmark current state-of-the-art of offline RL to robotic manipulation tasks, and conduct thorough ablations on what affects performance of policies learned offline. </p>
<p>From the pixel-map perspective, we could also cast the problem as a supervised learning task where we use labeled reward (or value) maps as an analogue to "expert data" in imitation learning.</p>
<h3 id="3-is-there-a-more-efficient-formulation-of-models-when-incorporating-more-motion-primitives"><a aria-hidden="true" href="#3-is-there-a-more-efficient-formulation-of-models-when-incorporating-more-motion-primitives" tabindex="-1"><span class="icon icon-link"></span></a>3) Is there a more efficient formulation of models when incorporating more motion primitives?</h3>
<p>The Pushing-Grasping paper explains as one of its future directions incorporating multiple types of motion primitives, such as rolling (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.02778em">r</span></span></span></span></span>), toppling (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.61508em;vertical-align:0"></span><span class="mord mathnormal">t</span></span></span></span></span>), squeezing (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">s</span></span></span></span></span>), leveraging (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.69444em;vertical-align:0"></span><span class="mord mathnormal" style="margin-right:.01968em">l</span></span></span></span></span>), stacking (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:.43056em;vertical-align:0"></span><span class="mord mathnormal">s</span></span></span></span></span>). Suppose, as an extreme case, we design a model that aims to learn the synergies between all of these. With the model proposed in the paper, a single MDP step would have to run the input state through 7 FCNs (for each motion primitive), and then find the pixel with the maximum q value of each output to determine the action. This would mean an increase in computational cost and time in between action selection, which could be problematic in real-world settings where low latency is crucial.</p>
<p>A more scalable approach could be inspired by hierarchical RL. For instance, if we have a meta-policy that maps state observations to motion-primitives, and then we obtain an action by running the observation through the sub-policy corresponding to a single motio-primitive. Such approach would only need run the input through two networks (meta-policy and chosen sub-policy) regardless of how many motion-primitives we incorporate. Some related papers that come to mind are [5, 6].</p>
<h3 id="references"><a aria-hidden="true" href="#references" tabindex="-1"><span class="icon icon-link"></span></a>References</h3>
<p>[1] Bellemare et al. A Distributional Perspective on Reinforcement Learning. In <em>Proceedings of the International Conference in Machine Learning,</em> 2017.</p>
<p>[2] Dabney et al. Distributional Reinforcement Learning with Quantile Regression. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2018.</p>
<p>[3] Dabney et al. Implicit Quantile Networks for Distributional Reinforcement Learning. In <em>Proceedings of International Conference in Machine Learning,</em> 2018. </p>
<p>[4] Levine et al. Offline Reinforcement Learning: Tutorial, Reivew, and Perspectives on Open Problems. 2020. </p>
<p>[5] Bacon et al. THe Option-Critic Architecture. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2016.</p>
<p>[6] Kulkarni et al. Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. In <em>Advances in Neural Information Processing Systems</em>, 2016.</p></div></article></main> <footer class="footer"></footer></div></div></div><script>window.__NUXT__=function(e,t,a,i,n,p,r,l,o,s,c,h,d,y,g,m,u,v,f,w,b,N,x,I,L,k,P,R,A,M,S,D,H,_,T,C,z,F,G){return{layout:"default",data:[{page:{title:"Follow-up Thoughts on a Couple Previous Publications from Dr. Shuran Song Lab.",datePublished:"September 3, 2020",abstract:'I have read the papers "Spatial Action Maps for Mobile Manipulation" and "Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning). I have summarized a few of my follow-up thoughts below.',imagePath:"placeholder_thumbnail.png",toc:[{id:A,depth:3,text:M},{id:S,depth:3,text:D},{id:H,depth:3,text:_},{id:T,depth:3,text:C}],body:{type:"root",children:[{type:e,tag:n,props:{},children:[{type:t,value:'I have read the papers "Spatial Action Maps for Mobile Manipulation" and "Learning Synergies between Pushing and Grasping with Self-supervised Deep Reinforcement Learning." I have summarized a few of my follow-up thoughts below.'}]},{type:t,value:i},{type:e,tag:k,props:{id:A},children:[{type:e,tag:P,props:{ariaHidden:r,href:"#1-distributional-value-approximations-embedded-into-pixel-wise-output-map",tabIndex:-1},children:[{type:e,tag:a,props:{className:[L,I]},children:[]}]},{type:t,value:M}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"Distributional value approximation [1, 2, 3] works great with deep reinforcement learning. I am wondering if these performance gains can be translated to learning pixel-wise value maps. If the added computational costs are too prohibitive (e.g. the output channel will now be multiple channels deep, each representing a single discretized unit of the value distribution), how can we make it more efficient? "}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:'Another reason for considering distributional value approximation is that, intuitively, it can introduce "risk sensitive" behavior, allowing agents to act conservatively when a certain state-action pair is estimated to have a high variance value distribution. To test this concept, how can we design similar motion-primitive or mobile navigation based tasks that involve risk sensitivity?'}]},{type:t,value:i},{type:e,tag:k,props:{id:S},children:[{type:e,tag:P,props:{ariaHidden:r,href:"#2-can-we-learn-synergies-offline",tabIndex:-1},children:[{type:e,tag:a,props:{className:[L,I]},children:[]}]},{type:t,value:D}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"I've noticed that offline RL has recently gained a lot of momentum in RL research [5]. Given a rich data set of transitions including multiple motion primitives, can a policy learn synergies between multiple motion primitives without any interaction with the environment (starting with simulations, but eventually moving onto sim-to-real)? "}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"It would be particularly interesting to benchmark current state-of-the-art of offline RL to robotic manipulation tasks, and conduct thorough ablations on what affects performance of policies learned offline. "}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:'From the pixel-map perspective, we could also cast the problem as a supervised learning task where we use labeled reward (or value) maps as an analogue to "expert data" in imitation learning.'}]},{type:t,value:i},{type:e,tag:k,props:{id:H},children:[{type:e,tag:P,props:{ariaHidden:r,href:"#3-is-there-a-more-efficient-formulation-of-models-when-incorporating-more-motion-primitives",tabIndex:-1},children:[{type:e,tag:a,props:{className:[L,I]},children:[]}]},{type:t,value:_}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"The Pushing-Grasping paper explains as one of its future directions incorporating multiple types of motion primitives, such as rolling ("},{type:e,tag:a,props:{className:[p,y]},children:[{type:e,tag:a,props:{className:[d]},children:[{type:e,tag:a,props:{className:[m]},children:[{type:e,tag:p,props:{xmlns:o},children:[{type:e,tag:v,props:{},children:[{type:e,tag:f,props:{},children:[{type:e,tag:w,props:{},children:[{type:t,value:"r"}]}]},{type:e,tag:N,props:{encoding:b},children:[{type:t,value:"r"}]}]}]}]},{type:e,tag:a,props:{className:[g],ariaHidden:r},children:[{type:e,tag:a,props:{className:[h]},children:[{type:e,tag:a,props:{className:[c],style:R},children:[]},{type:e,tag:a,props:{className:[u,s],style:"margin-right:0.02778em;"},children:[{type:t,value:"r"}]}]}]}]}]},{type:t,value:"), toppling ("},{type:e,tag:a,props:{className:[p,y]},children:[{type:e,tag:a,props:{className:[d]},children:[{type:e,tag:a,props:{className:[m]},children:[{type:e,tag:p,props:{xmlns:o},children:[{type:e,tag:v,props:{},children:[{type:e,tag:f,props:{},children:[{type:e,tag:w,props:{},children:[{type:t,value:"t"}]}]},{type:e,tag:N,props:{encoding:b},children:[{type:t,value:"t"}]}]}]}]},{type:e,tag:a,props:{className:[g],ariaHidden:r},children:[{type:e,tag:a,props:{className:[h]},children:[{type:e,tag:a,props:{className:[c],style:"height:0.61508em;vertical-align:0em;"},children:[]},{type:e,tag:a,props:{className:[u,s]},children:[{type:t,value:"t"}]}]}]}]}]},{type:t,value:"), squeezing ("},{type:e,tag:a,props:{className:[p,y]},children:[{type:e,tag:a,props:{className:[d]},children:[{type:e,tag:a,props:{className:[m]},children:[{type:e,tag:p,props:{xmlns:o},children:[{type:e,tag:v,props:{},children:[{type:e,tag:f,props:{},children:[{type:e,tag:w,props:{},children:[{type:t,value:l}]}]},{type:e,tag:N,props:{encoding:b},children:[{type:t,value:l}]}]}]}]},{type:e,tag:a,props:{className:[g],ariaHidden:r},children:[{type:e,tag:a,props:{className:[h]},children:[{type:e,tag:a,props:{className:[c],style:R},children:[]},{type:e,tag:a,props:{className:[u,s]},children:[{type:t,value:l}]}]}]}]}]},{type:t,value:"), leveraging ("},{type:e,tag:a,props:{className:[p,y]},children:[{type:e,tag:a,props:{className:[d]},children:[{type:e,tag:a,props:{className:[m]},children:[{type:e,tag:p,props:{xmlns:o},children:[{type:e,tag:v,props:{},children:[{type:e,tag:f,props:{},children:[{type:e,tag:w,props:{},children:[{type:t,value:"l"}]}]},{type:e,tag:N,props:{encoding:b},children:[{type:t,value:"l"}]}]}]}]},{type:e,tag:a,props:{className:[g],ariaHidden:r},children:[{type:e,tag:a,props:{className:[h]},children:[{type:e,tag:a,props:{className:[c],style:"height:0.69444em;vertical-align:0em;"},children:[]},{type:e,tag:a,props:{className:[u,s],style:"margin-right:0.01968em;"},children:[{type:t,value:"l"}]}]}]}]}]},{type:t,value:"), stacking ("},{type:e,tag:a,props:{className:[p,y]},children:[{type:e,tag:a,props:{className:[d]},children:[{type:e,tag:a,props:{className:[m]},children:[{type:e,tag:p,props:{xmlns:o},children:[{type:e,tag:v,props:{},children:[{type:e,tag:f,props:{},children:[{type:e,tag:w,props:{},children:[{type:t,value:l}]}]},{type:e,tag:N,props:{encoding:b},children:[{type:t,value:l}]}]}]}]},{type:e,tag:a,props:{className:[g],ariaHidden:r},children:[{type:e,tag:a,props:{className:[h]},children:[{type:e,tag:a,props:{className:[c],style:R},children:[]},{type:e,tag:a,props:{className:[u,s]},children:[{type:t,value:l}]}]}]}]}]},{type:t,value:"). Suppose, as an extreme case, we design a model that aims to learn the synergies between all of these. With the model proposed in the paper, a single MDP step would have to run the input state through 7 FCNs (for each motion primitive), and then find the pixel with the maximum q value of each output to determine the action. This would mean an increase in computational cost and time in between action selection, which could be problematic in real-world settings where low latency is crucial."}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"A more scalable approach could be inspired by hierarchical RL. For instance, if we have a meta-policy that maps state observations to motion-primitives, and then we obtain an action by running the observation through the sub-policy corresponding to a single motio-primitive. Such approach would only need run the input through two networks (meta-policy and chosen sub-policy) regardless of how many motion-primitives we incorporate. Some related papers that come to mind are [5, 6]."}]},{type:t,value:i},{type:e,tag:k,props:{id:T},children:[{type:e,tag:P,props:{ariaHidden:r,href:"#references",tabIndex:-1},children:[{type:e,tag:a,props:{className:[L,I]},children:[]}]},{type:t,value:C}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"[1] Bellemare et al. A Distributional Perspective on Reinforcement Learning. In "},{type:e,tag:x,props:{},children:[{type:t,value:"Proceedings of the International Conference in Machine Learning,"}]},{type:t,value:" 2017."}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"[2] Dabney et al. Distributional Reinforcement Learning with Quantile Regression. In "},{type:e,tag:x,props:{},children:[{type:t,value:z}]},{type:t,value:", 2018."}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"[3] Dabney et al. Implicit Quantile Networks for Distributional Reinforcement Learning. In "},{type:e,tag:x,props:{},children:[{type:t,value:"Proceedings of International Conference in Machine Learning,"}]},{type:t,value:" 2018. "}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"[4] Levine et al. Offline Reinforcement Learning: Tutorial, Reivew, and Perspectives on Open Problems. 2020. "}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"[5] Bacon et al. THe Option-Critic Architecture. In "},{type:e,tag:x,props:{},children:[{type:t,value:z}]},{type:t,value:F}]},{type:t,value:i},{type:e,tag:n,props:{},children:[{type:t,value:"[6] Kulkarni et al. Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation. In "},{type:e,tag:x,props:{},children:[{type:t,value:"Advances in Neural Information Processing Systems"}]},{type:t,value:F}]}]},dir:"/blog",path:G,extension:".md",slug:"_song_lab_application",createdAt:"2020-09-04T13:24:50.413Z",updatedAt:"2020-09-04T13:39:10.099Z"}}],fetch:[],error:null,serverRendered:!0,routePath:G}}("element","text","span","\n","p","math","true","s","http://www.w3.org/1998/Math/MathML","mathnormal","strut","base","katex","math-inline","katex-html","katex-mathml","mord","semantics","mrow","mi","application/x-tex","annotation","em","icon-link","icon","h3","a","height:0.43056em;vertical-align:0em;","1-distributional-value-approximations-embedded-into-pixel-wise-output-map","1) Distributional value approximations embedded into pixel-wise output map","2-can-we-learn-synergies-offline","2) Can we learn synergies offline?","3-is-there-a-more-efficient-formulation-of-models-when-incorporating-more-motion-primitives","3) Is there a more efficient formulation of models when incorporating more motion primitives?","references","References","Proceedings of the AAAI Conference on Artificial Intelligence",", 2016.","/blog/_song_lab_application")</script><script src="/_nuxt/149e7f33763fef6bbd60.js" defer></script><script src="/_nuxt/75fb1812f7eecc3449f1.js" defer></script><script src="/_nuxt/8df7c34f58155934d1b3.js" defer></script><script src="/_nuxt/2f2445117952219408c1.js" defer></script><script src="/_nuxt/d1d05cc850efa9e8bee5.js" defer></script>
  </body>
</html>
